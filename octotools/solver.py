# octotools/solver.py
import os
import time
from datetime import datetime
from typing import List, Optional, Dict, Any

from octotools.models.initializer import Initializer
from octotools.models.planner import Planner
from octotools.models.executor import SemanticExecutor
from octotools.models.memory import Memory
from octotools.models.semantic_registry import SemanticRegistry
from octotools.models.semantic_types import SemanticTypes
from octotools.models.utils import make_json_serializable_truncated

class SemanticSolver:
    """å‡çº§ç‰ˆè¯­ä¹‰æ„ŸçŸ¥æ±‚è§£å™¨"""
    
    def __init__(
        self,
        llm_engine_name: str,
        enabled_tools: List[str] = ["all"],
        output_types: str = "final,direct",
        max_steps: int = 10,
        max_time: int = 300,
        max_tokens: int = 4000,
        root_cache_dir: str = "solver_cache",
        verbose: bool = False
    ):
        self.llm_engine_name = llm_engine_name
        self.enabled_tools = enabled_tools
        self.output_types = output_types.split(",") if isinstance(output_types, str) else output_types
        self.max_steps = max_steps
        self.max_time = max_time
        self.max_tokens = max_tokens
        self.root_cache_dir = root_cache_dir
        self.verbose = verbose
        
        # åˆå§‹åŒ–å·¥å…·
        self.initializer = Initializer(
            enabled_tools=enabled_tools,
            model_string=llm_engine_name,
            verbose=verbose
        )
        
        # æ ¸å¿ƒç»„ä»¶
        self.semantic_registry = SemanticRegistry()
        self.memory = Memory()
        
        # åˆå§‹åŒ–è§„åˆ’å™¨
        self.planner = Planner(
            llm_engine_name=llm_engine_name,
            toolbox_metadata=self.initializer.toolbox_metadata,
            available_tools=self.initializer.available_tools,
            verbose=verbose
        )
        
        # åˆå§‹åŒ–æ‰§è¡Œå™¨
        self.executor = SemanticExecutor(
            llm_engine_name=llm_engine_name,
            root_cache_dir=root_cache_dir,
            max_time=max_time,
            verbose=verbose
        )
        
        # è¿æ¥è¯­ä¹‰æ³¨å†Œè¡¨
        self._connect_semantic_components()
        
        if verbose:
            print("ğŸ™ SemanticSolver initialized with semantic awareness")

    def _connect_semantic_components(self):
        """è¿æ¥è¯­ä¹‰åŒ–ç»„ä»¶ï¼Œç¡®ä¿å®ƒä»¬å…±äº«çŠ¶æ€"""
        # æ‰€æœ‰ç»„ä»¶å…±äº«åŒä¸€ä¸ªè¯­ä¹‰æ³¨å†Œè¡¨
        self.planner.set_semantic_registry(self.semantic_registry)
        self.executor.semantic_registry = self.semantic_registry
        
        # ç¡®ä¿verboseè®¾ç½®ä¸€è‡´
        if hasattr(self.semantic_registry, 'verbose'):
            self.semantic_registry.verbose = self.verbose

    def solve(self, question: str, image: str = "", files: List[str] = None) -> str:
        """ä¸»æ±‚è§£æ–¹æ³• - è¯­ä¹‰æ„ŸçŸ¥ç‰ˆæœ¬"""
        
        if self.verbose:
            print(f"\n==> ğŸ” Received Query: {question}")
        
        # è®¾ç½®æŸ¥è¯¢ç¼“å­˜ç›®å½•
        query_cache_dir = self._setup_query_cache()
        self.executor.set_query_cache_dir(query_cache_dir)
        
        # åˆå§‹åŒ–å†…å­˜
        self.memory.set_query(question)
        if files:
            self.memory.add_file(files)
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: æŸ¥è¯¢åˆ†æ
            if self.verbose:
                print(f"\n==> ğŸ™ Reasoning Steps from OctoTools (Deep Thinking...)")
                print(f"\n==> ğŸ” Step 0: Query Analysis")
            
            query_analysis_start = time.time()
            query_analysis = self.planner.analyze_query(question, image)
            
            if self.verbose:
                print(query_analysis)
                print(f"[Time]: {time.time() - query_analysis_start:.2f}s")
            
            # ä¸»å¾ªç¯ï¼šè¯­ä¹‰æ„ŸçŸ¥çš„æ­¥éª¤æ‰§è¡Œ
            step_count = 1
            while step_count <= self.max_steps:
                
                # æ£€æŸ¥æ—¶é—´é™åˆ¶
                if time.time() - start_time > self.max_time:
                    if self.verbose:
                        print(f"\n==> â° Time limit reached ({self.max_time}s)")
                    break
                
                # ç”Ÿæˆä¸‹ä¸€æ­¥
                step_start_time = time.time()
                if self.verbose:
                    print(f"\n==> ğŸ¯ Step {step_count}: Action Prediction")
                
                next_step = self.planner.generate_next_step(
                    question, image, query_analysis, self.memory, step_count, self.max_steps
                )
                
                # æå–æ­¥éª¤ä¿¡æ¯
                context, sub_goal, tool_name = self.planner.extract_context_subgoal_and_tool(next_step)
                
                if tool_name is None or "No matched tool" in tool_name:
                    if self.verbose:
                        print(f"\n==> âŒ Invalid tool selection: {tool_name}")
                    break
                
                if self.verbose:
                    print(f"\n[Context]: {context}")
                    print(f"[Sub Goal]: {sub_goal}")
                    print(f"[Tool]: {tool_name}")
                    print(f"[Time]: {time.time() - step_start_time:.2f}s")
                    
                    # æ˜¾ç¤ºå½“å‰è¯­ä¹‰çŠ¶æ€
                    semantic_status = self.semantic_registry.get_completion_status()
                    completed_types = [k for k, v in semantic_status.items() if v]
                    if completed_types:
                        print(f"[Semantic Progress]: {completed_types}")
                
                # ç”Ÿæˆå·¥å…·å‘½ä»¤
                if self.verbose:
                    print(f"\n==> ğŸ“ Step {step_count}: Command Generation ({tool_name})")
                
                command_start = time.time()
                tool_metadata = self.initializer.toolbox_metadata.get(tool_name, {})
                
                tool_command = self.executor.generate_tool_command(
                    question, image, context, sub_goal, tool_name, tool_metadata
                )
                
                analysis, explanation, command = self.executor.extract_explanation_and_command(tool_command)
                
                if self.verbose:
                    print(f"\n[Analysis]: {analysis}")
                    print(f"[Explanation]: {explanation}")
                    print(f"[Command]: {command}")
                    print(f"[Time]: {time.time() - command_start:.2f}s")
                
                # æ‰§è¡Œå‘½ä»¤
                if self.verbose:
                    print(f"\n==> ğŸ› ï¸ Step {step_count}: Command Execution ({tool_name})")
                
                execution_start = time.time()
                
                # æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
                if tool_name not in self.initializer.available_tools:
                    if self.verbose:
                        print(f"\n==> ğŸš« Error: Tool '{tool_name}' is not available or not found.")
                    break
                
                # æ‰§è¡Œå·¥å…·å‘½ä»¤
                executions = self.executor.execute_tool_command(tool_name, command)
                
                if self.verbose:
                    print(f"\n[Result]:")
                    formatted_result = make_json_serializable_truncated(executions, max_length=1000)
                    print(formatted_result)
                    print(f"[Time]: {time.time() - execution_start:.2f}s")
                
                # æ·»åŠ åˆ°å†…å­˜
                self.memory.add_action(step_count, tool_name, sub_goal, command, executions)
                
                # éªŒè¯ä¸Šä¸‹æ–‡å¹¶å†³å®šæ˜¯å¦ç»§ç»­
                if self.verbose:
                    print(f"\n==> ğŸ¤– Step {step_count}: Context Verification")
                
                verification_start = time.time()
                
                verification = self.planner.verificate_context(question, image, query_analysis, self.memory)
                analysis, conclusion = self.planner.extract_conclusion(verification)
                
                if self.verbose:
                    print(f"\n[Analysis]: {analysis}")
                    print(f"[Conclusion]: {conclusion} {'ğŸ›‘' if conclusion == 'CONTINUE' else 'âœ…'}")
                    print(f"[Time]: {time.time() - verification_start:.2f}s")
                
                # æ˜¾ç¤ºè¯­ä¹‰è¿›åº¦æŠ¥å‘Š
                if self.verbose and hasattr(self.memory, 'generate_workflow_report'):
                    print(f"\n[Semantic Status]:")
                    print(self.memory.generate_workflow_report())
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if conclusion == 'STOP':
                    if self.verbose:
                        print(f"\n==> âœ… Context verification complete. Proceeding to final output.")
                    break
                
                step_count += 1
            
            # ç”Ÿæˆæœ€ç»ˆè¾“å‡º
            if self.verbose:
                print(f"\n==> ğŸ™ Final Answer:")
            
            final_output = self._generate_final_output(question, image)
            
            if self.verbose:
                total_time = time.time() - start_time
                print(f"\n[Total Time]: {total_time:.2f}s")
                print(f"\n==> âœ… Query Solved!")
            
            return final_output
            
        except Exception as e:
            error_msg = f"Error during solving: {str(e)}"
            if self.verbose:
                print(f"\n==> âŒ {error_msg}")
            return error_msg

    def _setup_query_cache(self) -> str:
        """è®¾ç½®æŸ¥è¯¢ç¼“å­˜ç›®å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_cache_dir = os.path.join(self.root_cache_dir, timestamp)
        os.makedirs(query_cache_dir, exist_ok=True)
        return query_cache_dir

    def _generate_final_output(self, question: str, image: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
        if "direct" in self.output_types:
            return self.planner.generate_direct_output(question, image, self.memory)
        elif "final" in self.output_types:
            return self.planner.generate_final_output(question, image, self.memory)
        else:
            return self.planner.generate_direct_output(question, image, self.memory)

    def get_semantic_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰è¯­ä¹‰çŠ¶æ€"""
        return {
            "semantic_registry": self.semantic_registry.get_completion_status(),
            "memory_progress": self.memory.get_workflow_progress() if hasattr(self.memory, 'get_workflow_progress') else {},
            "available_fragments": list(self.semantic_registry.fragments.keys()),
            "next_possible": self.semantic_registry.get_next_possible_types(),
            "executor_variables": list(self.executor.global_variables.keys())
        }

    def export_solution(self, output_dir: str = ".") -> Dict[str, str]:
        """å¯¼å‡ºå®Œæ•´è§£å†³æ–¹æ¡ˆ"""
        results = {}
        
        # å¯¼å‡ºè¯­ä¹‰ç‰‡æ®µ
        if hasattr(self.memory, 'export_semantic_fragments'):
            fragments_dir = self.memory.export_semantic_fragments(output_dir)
            results["fragments_directory"] = fragments_dir
        
        # å¯¼å‡ºå®Œæ•´è§£å†³æ–¹æ¡ˆ
        complete_solution = self.semantic_registry.get_fragment(SemanticTypes.COMPLETE_SOLUTION)
        if complete_solution:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            solution_file = os.path.join(output_dir, f"complete_solution_{timestamp}.py")
            
            with open(solution_file, 'w', encoding='utf-8') as f:
                f.write(complete_solution.code)
            
            results["solution_file"] = solution_file
        
        # å¯¼å‡ºè¯­ä¹‰çŠ¶æ€æŠ¥å‘Š
        status_file = os.path.join(output_dir, f"semantic_status_{timestamp}.json")
        with open(status_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(self.get_semantic_status(), f, indent=2, default=str)
        results["status_file"] = status_file
        
        return results

# å‘åå…¼å®¹ï¼šä¿æŒåŸæœ‰çš„æ„é€ å‡½æ•°
def construct_solver(
    llm_engine_name: str,
    enabled_tools: List[str] = ["all"],
    output_types: str = "final,direct",
    max_steps: int = 10,
    max_time: int = 300,
    max_tokens: int = 4000,
    root_cache_dir: str = "solver_cache",
    verbose: bool = False
) -> SemanticSolver:
    """æ„é€ è¯­ä¹‰æ„ŸçŸ¥æ±‚è§£å™¨ï¼ˆä¿æŒå‘åå…¼å®¹çš„æ¥å£ï¼‰"""
    
    return SemanticSolver(
        llm_engine_name=llm_engine_name,
        enabled_tools=enabled_tools,
        output_types=output_types,
        max_steps=max_steps,
        max_time=max_time,
        max_tokens=max_tokens,
        root_cache_dir=root_cache_dir,
        verbose=verbose
    )

# ä¸ºäº†å®Œå…¨å‘åå…¼å®¹ï¼Œä¹Ÿå¯ä»¥ä¿ç•™æ—§çš„ç±»å
class Solver(SemanticSolver):
    """å‘åå…¼å®¹çš„æ±‚è§£å™¨åˆ«å"""
    pass